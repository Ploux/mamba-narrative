Gu, A., & Dao, T. (2024, May). Mamba: Linear-time sequence modeling with selective state spaces. In _First conference on language modeling_. https://arxiv.org/abs/2312.00752

https://arxiv.org/html/2508.10824v1 Memory-Augmented Transformers: A Systematic Review from Neuroscience Principles to Technical Solutions

https://arxiv.org/html/2311.12351v2 Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey

[1] EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models https://arxiv.org/abs/2312.06281

[2] AI-judged writing metrics by author of EQ-Bench paper https://github.com/EQ-bench/creative-writing-bench

[3] https://www.microsoft.com/en-us/research/blog/advances-to-low-bit-quantization-enable-llms-on-edge-devices/ Microsoft's T-MAC demonstrated that low-bit LLMs can achieve 48 tokens/second for 3B BitNet models on Surface Laptop devices, and 11 tokens/second on Raspberry Pi 5, proving edge deployment viability

[4] https://dl.acm.org/doi/10.1145/3649329.3658473 EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Unified Compression and Adaptive Layer Voting

[5] Are Large Language Models Capable of Generating Human-Level Narratives? https://arxiv.org/abs/2407.13248 (comparing story arcs)

[6] Art or Artifice? Large Language Models and the False Promise of Creativity (https://arxiv.org/pdf/2309.14556.pdf) (human evaluators rate AI stories as inferior)

[7] https://arxiv.org/html/2409.11547v2 (Small Language Models can Outperform Humans in Short Creative Writing: A Study Comparing SLMs with Humans and LLMs) (only movie synopses)

[8] https://arxiv.org/html/2506.08172 GrAImes: an evaluation protocol grounded in literary theory (bizarre evaluation criteria)

[9] https://arxiv.org/html/2510.02025 Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models (giving models constraints)

[10] Evaluating Creative Short Story Generation in Humans and Large Language Models https://arxiv.org/abs/2411.02316 (only five sentence stories)

[11] Creating Suspenseful Stories: Iterative Planning with Large Language Models https://aclanthology.org/2024.eacl-long.147/ (zero-shot prompting)

[12] A Theoretical Framework for Evaluating Narrative Surprise in Large Language Models  https://aclanthology.org/2025.wnu-1.7/ (evaluation of LLM writing)

[13] https://github.com/karpathy/nanochat (training a weak ChatGPT clone with $100 of compute time)
